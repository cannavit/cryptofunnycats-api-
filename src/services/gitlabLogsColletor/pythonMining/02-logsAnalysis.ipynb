{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# (3) Logs Analysis using Mining. \n",
                "# Data set type: Industial-Anoki\n",
                "\n",
                "# ðŸ’¨ðŸ”¥ðŸ’¨ Smoke Analysis\n",
                "\n",
                "#### âœ… python, âœ… Gitlab, âœ… Mongodb\n",
                "\n",
                "## Qs\n",
                "### Q1: What is the top of most common problems in pipelines?\n",
                "\n",
                "## Index\n",
                "\n",
                "- [Import python libraries](#Import-python-libraries)\n",
                "- [Page reference](#Page-reference)\n",
                "- [Create event list](#Create-event-list)\n",
                "- [Group similar text](#Group-similar-text)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Nomenclature\n",
                "    - (STPS) Smoke test possible solution: It is the set of tentative errors that can be avoided by using smoke tests"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Import python libraries"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import os\n",
                "import pymongo\n",
                "from pymongo import MongoClient\n",
                "from datetime import date\n",
                "# Tratamiento de datos\n",
                "# ==============================================================================\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import string\n",
                "import re\n",
                "# GrÃ¡ficos\n",
                "# ==============================================================================\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import style\n",
                "import seaborn as sns\n",
                "# Plotly\n",
                "import plotly.express as px\n",
                "from plotly.subplots import make_subplots\n",
                "import plotly.graph_objects as go\n",
                "import plotly.io as pio\n",
                "pio.renderers.default='notebook'\n",
                "# ==============================================================================\n",
                "from sklearn import svm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt') # first-time use only\n",
                "nltk.download('wordnet') # first-time use only\n",
                "from nltk.corpus import stopwords\n",
                "import string\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "\n",
                "import math\n",
                "# ConfiguraciÃ³n warnings\n",
                "# ==============================================================================\n",
                "import warnings"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     /Users/ceciliocannavaciuolo/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     /Users/ceciliocannavaciuolo/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     /Users/ceciliocannavaciuolo/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load variables to perform the analysis"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Smoke Test Parameters\n",
                "# ==============================================================================\n",
                "# Plotly configuration\n",
                "plotly_template=\"plotly_dark\"\n",
                "# plotly_template=\"plotly\"\n",
                "# Filters:\n",
                "JobsNameBlackList = ['test'] # Remove all jobs of analysis with this name\n",
                "logsWhiteList = [\"error\",\"fail\", \"warning\"] # Check lines of logs with this words\n",
                "JobsStatusWhiteList = [\"failed\"] # Check jobs with this status \n",
                "similarity = 0.6 # Group similar text\n",
                "# DATA:\n",
                "# mongoDbLimit=1000 # Limit of data request in mongodb\n",
                "mongoDbLimit=False # Limit of data request in mongodb\n",
                "csvRead=True\n",
                "csvSave=True\n",
                "\n",
                "csvName=\"data-02-logsAnalysis-\" # CSV file name\n",
                "csvFileRead=\"data-02-logsAnalysis-13-08-2021.csv\"\n",
                "pathExperimentsFiles=\"/Users/ceciliocannavaciuolo/Documents/workspace/phd/experimentsGitlabColellector\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# References and Pages"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "- [TextBlob: Librarian documentation to do text analysis](https://textblob.readthedodfdfdfio/en/dev/quicksdfadftdfhtml#get-word-and-noun-phrase-frequencies)\n",
                "- pipenv: https://pipenv.pydfdfdfio/en/latest/\n",
                "- pythonplot: https://pythonplot.com/\n",
                "- [Measuring Similarity Between Texts in Python](https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/)\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Collect data from MongoDB\n",
                "### Read data from Mongodb database"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# Connect with DB\n",
                "if not csvRead:\n",
                "    MONGODB_URL = os.environ.get('MONGODB_URL')\n",
                "    NODE_ENV = os.environ.get('NODE_ENV') or \"dev\"\n",
                "    DB_NAME = os.environ.get('APP_NAME') + \"-\"+ NODE_ENV\n",
                "\n",
                "    client = MongoClient()\n",
                "    client = MongoClient(MONGODB_URL)\n",
                "    db = client[DB_NAME]\n",
                "\n",
                "    if mongoDbLimit:\n",
                "        jobs = db.gitlablogs.find({}).limit(mongoDbLimit) # Read all data\n",
                "    else:\n",
                "        print(\"@Note-01 ---- 2018711928 -----\")\n",
                "        jobs = db.gitlablogs.find({}) # Read all data\n",
                "        \n",
                "    jobs = pd.DataFrame(list(jobs)) # Convert to DataFrame\n",
                "    print(\"List of data available iside of db structure\")\n",
                "\n",
                "    # Save CSV\n",
                "    if csvSave:\n",
                "        today = date.today()\n",
                "        today = today.strftime(\"%d-%m-%Y\")\n",
                "        jobs.to_csv(pathExperimentsFiles+'/dataAnalysis/'+csvName+today+'.csv', index = False)\n",
                "else:\n",
                "    print(\"@Note-01 ---- 1350489220 -----\")\n",
                "    print(pathExperimentsFiles+'/dataAnalysis/'+csvFileRead)\n",
                "    jobs = pd.read_csv(pathExperimentsFiles+'/dataAnalysis/'+csvFileRead)\n",
                "\n",
                "jobs.dtypes"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "@Note-01 ---- 1350489220 -----\n",
                        "/Users/ceciliocannavaciuolo/Documents/workspace/phd/experimentsGitlabColellector/dataAnalysis/data-02-logsAnalysis13-08-2021.csv\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: '/Users/ceciliocannavaciuolo/Documents/workspace/phd/experimentsGitlabColellector/dataAnalysis/data-02-logsAnalysis13-08-2021.csv'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/var/folders/3m/yp4nwdq15ds_j0ywspwlwjnc0000gn/T/ipykernel_34718/651329840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@Note-01 ---- 1350489220 -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathExperimentsFiles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/dataAnalysis/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcsvFileRead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathExperimentsFiles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/dataAnalysis/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcsvFileRead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ceciliocannavaciuolo/Documents/workspace/phd/experimentsGitlabColellector/dataAnalysis/data-02-logsAnalysis13-08-2021.csv'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Analysis of data volumes.\n",
                "## In this section you can obtain general information related to the volume of data.\n",
                "## Percentage of type jobs"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(\"------ DATA REPORT ------\")\n",
                "projectsNumber = len(jobs[\"projectName\"].unique())\n",
                "print(\"Number of Projects: \"+ str(projectsNumber))\n",
                "numberOfJobs = len(jobs.index)\n",
                "print(\"Number of pipelines Jobs (Steps): \"+ str(numberOfJobs))\n",
                "numberOfSuccess= jobs.loc[jobs[\"jobStatus\"] == \"success\"][\"jobStatus\"].count()\n",
                "numberOfFailed= jobs.loc[jobs[\"jobStatus\"] == \"failed\"][\"jobStatus\"].count()\n",
                "numberOfCancel= jobs.loc[jobs[\"jobStatus\"] == \"canceled\"][\"jobStatus\"].count()\n",
                "\n",
                "successPercentage = (1-((numberOfSuccess+numberOfFailed+numberOfCancel)-numberOfSuccess)/(numberOfSuccess+numberOfFailed+numberOfCancel))*100\n",
                "failedPercentage = (1-((numberOfSuccess+numberOfFailed+numberOfCancel)-numberOfFailed)/(numberOfSuccess+numberOfFailed+numberOfCancel))*100\n",
                "canceledPercentage = (1-((numberOfSuccess+numberOfFailed+numberOfCancel)-numberOfCancel)/(numberOfSuccess+numberOfFailed+numberOfCancel))*100\n",
                "\n",
                "print(\"Number of success Jobs (Steps): \"+ str(numberOfSuccess) + \" or \"+str(successPercentage) + \" %\")\n",
                "print(\"Number of failed Jobs (Steps): \"+ str(numberOfFailed)+ \" or \"+str(failedPercentage) + \" %\")\n",
                "print(\"Number of canceled Jobs (Steps): \"+ str(numberOfCancel)+ \" or \"+str(canceledPercentage) + \" %\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "fig = make_subplots(rows=1, cols=2)\n",
                "fig = px.pie(jobs, names='jobStatus', title='Pipelines Jobs results',color=\"jobStatus\",template=plotly_template)\n",
                "fig.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Number of fails by stage number"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def createBarGraphByJobStatus(variable):\n",
                "\n",
                "    jobStatusUnique = jobs[\"jobStatus\"].unique().tolist()\n",
                "    df_list = []\n",
                "    for status in jobStatusUnique:\n",
                "        jobs_total = jobs.rename(columns={'jobStatus': status})\n",
                "        total = jobs_total.groupby(by=variable)[status].count()\n",
                "        df_list.append(total)\n",
                "\n",
                "    df = pd.concat(df_list,axis=1)\n",
                "    df = df.sort_values(by=[jobStatusUnique[0]],ascending=True)\n",
                "    fig = px.bar(df, orientation='h',template=plotly_template,title=\"Number of fails by \"+ variable)\n",
                "    fig.show()\n",
                "    \n",
                "print(\" Number of jobs projectName types\")\n",
                "createBarGraphByJobStatus(\"projectName\")    \n",
                "print(\" Number of jobs stage types\")\n",
                "createBarGraphByJobStatus(\"jobStage\")\n",
                "createBarGraphByJobStatus(\"pipelineRef\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Filter logs data\n",
                "## Get fragment of text with error "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Get fragment of text with error\n",
                "# ==============================================================================\n",
                "def getErrorText(texto):\n",
                "    #! Get only last range. \n",
                "    nuevo_texto = texto\n",
                "    #! Convert all text to lowercase.\n",
                "\n",
                "    try:\n",
                "        nuevo_texto = nuevo_texto.lower()\n",
                "    except:\n",
                "        return \"\"\n",
                "\n",
                "    #! Web page removal (words beginning with \"http\")\n",
                "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
                "    nuevo_texto = nuevo_texto.split(sep = '\\n');\n",
                "    \n",
                "    whiteList = [\"error\"]; newTextList = []; nuevoTexto = \"\"\n",
                "    for text in nuevo_texto:\n",
                "        for listI in whiteList:\n",
                "            if listI in text:\n",
                "                #! Remove special characters\n",
                "                regex = '[\\\\!\\\\\"\\\\#\\\\>\\\\<\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\;\\\\\\\\\\]\\\\<\\\\=\\\\,\\\\>\\\\?\\\\:\\\\-\\\\|\\\\@\\\\@\\\\\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
                "                text = re.sub(regex , ' ', text)\n",
                "\n",
                "                text = re.sub('http\\S+', ' ', text)\n",
                "\n",
                "                #! Remove numbers\n",
                "                text = re.sub('\\d+', ' ', text)\n",
                "                #! remove date\n",
                "                text = re.sub('\\d{4}-\\d{2}-\\d{2}', ' ', text)\n",
                "                text = re.sub(' +', ' ', text)\n",
                "                text = re.sub('- - t : :','',text)\n",
                "                #! Removing emojis\n",
                "                emoji_pattern = re.compile(\"[\"\n",
                "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
                "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
                "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
                "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
                "                           \"]+\", flags=re.UNICODE)\n",
                "                text = emoji_pattern.sub(r'', text)\n",
                "                \n",
                "                if not text in newTextList:\n",
                "                    #! cut string. \n",
                "                    text = text.strip()\n",
                "                    text.replace('[','').replace(']','')\n",
                "                    text = text[0:text.find(\".\")]\n",
                "                    nuevoTexto = nuevoTexto + \"\\n\" + text\n",
                "\n",
                "                newTextList.append(text)\n",
                "\n",
                "                newTextList = [string for string in newTextList if string.strip()]# Remove multiple empty spaces from string List\n",
                "                newTextList = [string for string in newTextList if string != \" \"] # Delete empty strings\n",
                "                newTextList = [string for string in newTextList if string != \"\"] # Delete empty strings  \n",
                "                newTextList = [string for string in newTextList if (len(string) > 2)] # EliminaciÃ³n de tokens con una longitud < 2 o que se encuentren en la lista de palabras ignoradas\n",
                "    \n",
                "                unics = set(); newTextList = [string for string in newTextList if string not in unics and (unics.add(string) or True)] # Delete duplicate data\n",
                "\n",
                "    return newTextList"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Test Filter for search the text error inside of the logs"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "textExample = '''\n",
                " * [new branch]      frontend-test           -> origin/frontend-test\n",
                " * [new branch]      functional-testing-junit -> origin/functional-testing-junit\n",
                " * [new branch]      master                  -> origin/master\n",
                " * [new branch]      sol-dev                 -> origin/sol-dev\n",
                " * [new branch]      sql-fix-branch          -> origin/sql-fix-branch\n",
                " * [new branch]      testing-report          -> origin/testing-report\n",
                " * [new tag]         CR-H1-2021-deploy-intermedio -> CR-H1-2021-deploy-intermedio\n",
                " * [new tag]         v1dffdfdfdf0_20190405         -df dff1dff0df0_20190405\n",
                " * [new tag]         v1dffdfdfdf1_20190424         -df dff1dff0df1_20190424\n",
                "Checking out 18b927c2 as devdfdfdffdffdf\n",
                "\n",
                "Skipping Git submodules setup\n",
                "Downloading artifacts for build-jar (1473723797)dfdfdffdffdf\n",
                "Downloading artifacts from coordinatordfdfdffdffdf ok        id=1473723797 responseStatus=200 OK token=G4kf2uft\n",
                "$ echo \"{\\\"auths\\\":{\\\"$CI_REGISTRY\\\":{\\\"username\\\":\\\"$CI_REGISTRY_USER\\\",\\\"password\\\":\\\"$CI_REGISTRY_PASSWORD\\\"}}}\" > /kaniko/.docker/confdfdfdfjson\n",
                "$ /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG\n",
                "INFO[0001] Retrieving image manifest openjdk:11.0       \n",
                "INFO[0001] Retrieving image openjdk:11.0 from registry inddfdfdfdodfkdfrdfio \n",
                "error building image: GET https://index.dockdfdfdfio/v2/library/openjdk/manifestdf/df1df0: TOOMANYREQUESTS: You have reached your pull ratedflidfitdf You may increase the limit by authenticating and upgrading: httdfs:/dfwdfwdfddfckerdfcom/increase-rate-limit\n",
                "ERROR: Job failed: command terminated with exit code 1\n",
                "'''\n",
                "\n",
                "errorText =  getErrorText(textExample)\n",
                "print(errorText)\n",
                "# This need to do defferent to 0.  \n",
                "assert  len(errorText) > 0\n",
                "print(\"âœ… PASS TEST: OK ðŸ‘\")\n",
                "\n",
                "textExample = '''\n",
                "    >>>> 03_add_account_toEntity-06 :              |'\\n  â”‚ '|------------------------------------------------|'\\n  â”‚ \\n  â”‚ 'https://edutelling-api-develop.openshidfdfdftecdfgdfpdfit/a\\n  â”‚ pi/v1/tutors/create/tutor-from-ambassador?ambassadorId\\n  â”‚ ='\\n  â”‚ 'RESPONSE'\\n  â”‚ \\n  â”‚ `{\\\"success\\\":true,\\\"message\\\":\\\"Tutor '#17:-2' cdfeadfeddf\\\",\\\"\\n  â”‚ data\\\":{\\\"tutorId\\\":\\\"#17:-2\\\"}}`\\n  â”‚ '|-**********************************************-|'\\n  â”‚ \\n  â””\\n\\nâ†’ 04_login_no2_v3\\n  POST https://edutelling-apidfdevdflopddfopendfhdfftdftdfchgapdfit/api/v1/auth/authentication [200 OK, 798B, 137ms]\\n  âœ“  [(POST)/api/v1/auth/authentication] Login Delete (200)\\n\\nâ†’ 05_login_switch_how_account\\n  POST https://edutellidfg-api-dfedfelopdfodffenshiftdfftechgapdfit/api/v1/auth/complete-authedftication df200 OK, 1df01KB, 27ms]\\n  âœ“  CHECK IF EXIST JWT\\n  âœ“  CHECK IF EXIST jwtRefresh\\n  âœ“  [(POST)/api/v1/auth/authentication] Login Delete (200)\\n\\nâ†’ 01_entityTypeAccountService_getAll\\n  GET https://dfdutellingdfdfpi-dedfelodfdfopensdfiftdftechgapdfit/apdf/v1/accounts/dfmbassador@botdfcom/all [200 OK, 680B, 23ms]\\n  â”Œ\\n  â”‚ 'deleteAccountId :', '#12:1564'\\n  â”‚ 'deleteAccountId :', '12%3A1564'\\n  â””\\n\\nâ†’ 02_reamoveAccount\\n  DELETE dfttps://eddftelldfndf-api-dedfelopdfodfenshiftdftechgapdfit/api/v1/accounts/remove?dataId=12%3A1564 [200 OK, 397B, 15ms]\\n\\nâ†’ goTo_workflowControl\\n  OPdfIONS httpdf://edutdfflling-apidfdevelopdffopenshiftdftechgapdfit [404 Not Found, 255B, 4ms]\\n\\nAttempting to set next request to controlOfWorkFlowTempalte_06\\n\\nâ†’ controlOfWorkFlowTempalte_06dfn  OPTIONdf https:df/edfutelling-dfpi-devedfopdfopenshiftdftechgapdfit [404 Not Found, 255B, 6ms]\\n  â”Œ\\n  â”‚ ' ------ ------ ---------- ---------'\\n  â”‚ \\n  â”‚ '{\\\"nameOfConfigFile\\\":\\\"config_multiAccount_tutorHowAmba\\n  â”‚ ssador\\\",\\\"templateRun\\\":[\\\"01_entityTypeAccountService_ge\\n  â”‚ tAll_06\\\",\\\"config_multiAccount_studentHowAmbassador\\\"],\\\"\\n  â”‚ executed\\\":[false,true]}'\\n  â”‚ \\n  â”‚ 'WORKFLOW GO TO: ------------------------'\\n  â”‚ \\n  â”‚ 'WORKFLOW CONTROL REDIRECTING TEMPATE TO: ', 'config_m\\n  â”‚ ultiAccount_studentHowAmbassador'\\n  â”‚ \\n  â”‚ \\n  â””\\n\\nAttempting to set next request to config_multiAccount_studentHowAmbassador\\n\\nâ†’ config_multiAccount_studentHowAmbadfsador\\n  dfPTIONS dfttps:df/edutellidfg-api-ddfvelopdfopenshiftdftechgapdfit [404 Not Found, 255B, 5ms]\\n\\nAttempting to set next request to controlOfWorkFlowTempalte_06\\n\\nâ†’ controlOfWorkFlodfTempalte_df6\\n  OPdfIONS httdfs://edutedfling-apdf-developdfopenshiftdftechgapdfit [404 Not Found, 255B, 8ms]\\n  â”Œ\\n  â”‚ ' ------ ------ ---------- ---------'\\n  â”‚ \\n  â”‚ '{\\\"nameOfConfigFile\\\":\\\"config_multiAccount_tutorHowAmba\\n  â”‚ ssador\\\",\\\"templateRun\\\":[\\\"01_entityTypeAccountService_ge\\n  â”‚ tAll_06\\\",\\\"stop_06_createCourseModuleAndStage\\\"]}'\\n  â”‚ \\n  â”‚ 'WORKFLOW GO TO: ------------------------'\\n  â”‚ \\n  â”‚ 'WORKFLOW CONTROL REDIRECTING TEMPATE TO: ', '01_entit\\n  â”‚ yTypeAccountService_getAll_06'\\n  â”‚ \\n  â”‚ \\n  â””\\n\\nAttempting to set next request to 01_entityTypeAccountService_getAll_06\\n\\nâ†’ 01_entityTypeAcdfountServidfe_getAldf_06\\n  GET dfttps://eddftellindfdfapi-developdfopenshiftdftechgadfdfit/api/v1/accounts/student@botdfcom/all [200 OK, 475B, 8ms]\\n  â”Œ\\n  â”‚ 'entityTypeUppperCase: ', 'Student'\\n  â”‚ '20%3A273'\\n  df\\n\\nâ†’ 02_dfeamoveAdfcount_06\\n  DELdfTE https:df/eduteldfing-api-developdfopenshiftdftechgapdfit/api/v1/accounts/remove?dataId=12%3A1564 [200 OK, 379B, 53dfs]\\n\\nâ†’ 0df_add_acdfount_toEntity-06\\ndf POST httdfs://edudfelling-api-developdfopenshiftdftechgapdfit/api/v1/ambassadors/create/ambassador-from-student?studentId=20%3A273 [200 OK, 420B, 84ms]\\n  âœ“   [(POST) /api/v1/teachers/create/teacher-from-student?studentId ] Status code is 200\\n  âœ“  Check if was Success to add access student how teacher\\n  â”Œ\\n  â”‚ \\n  â”‚ '|------------------------------------------------|'\\n  â”‚ '| >>>> 03_add_account_toEntity-06 :              |'\\n  â”‚ '|-------------df---------df-------df----------------|'\\n dfâ”‚ \\n  â”‚ 'dfttps://dfdutelling-api-developdfopenshiftdftechgapdfit/a\\n  â”‚ pi/v1/ambassadors/create/ambassador-from-student?stude\\n  â”‚ ntId='\\n  â”‚ 'RdfSPONSE'\\n  â”‚ \\n  â”‚ `{\\\"success\\\":true,\\\"medfsage\\\":\\\"Ambassador '#19:-2' create\\n  â”‚ ddf\\\",\\\"data\\\":{\\\"ambassadorId\\\":\\\"#19:-2\\\"}}`\\n  â”‚ '|-********************************df*********df***-|'\\df  â”‚ \\n  â””\\n\\nâ†’ 04_login_ndf2_v3\\n  PdfST httpdf://edutelling-api-developdfopenshiftdftechgapdfit/api/v1/auth/authentication [200 OK, 790B, 152ms]\\n  âœ“  [(POST)/api/v1/auth/authenticatidfn] Login dfelete (df00)\\n\\nâ†’ 05_login_switch_howdfaccount\\ndf POST hddftps://edutelling-api-developdfopenshiftdftechgapddfit/api/v1/auth/complete-authentication [200 OK, 1df03KB, 23ms]\\n  âœ“  CHECK IF EXIST JWT\\n  âœ“  CHECK IF EXIST jwtRefresh\\n  âœ“  [(POST)/api/v1/auth/authenticadfion] Logidf Deletedf(200)\\n\\nâ†’ 01_entityTypeAccoundfSdfrvice_getdfll\\n  GdfT https://edutelling-api-develdfpdfopenshiftdftechgapdfit/api/v1/accounts/student@botdfcom/all [200 OK, 676B, 9ms]\\n  â”Œ\\n  â”‚ 'deleteAccountId :', '#12:15df5'\\n  â”‚ 'dfeleteAcdfountId :', '12%3A1565'\\n  â””\\n\\nâ†’ 02_dfeamoveAccdfunt\\n  dfELETE https://edutelling-api-developdfopenshiftdftechgapdfit/api/v1/accoudfts/removedfdataId=df2%3A1565 [200 OK, 397B, 37ms]\\n\\nâ†’ goTodfworkflowCdfntrol\\ndf OPTIONS https://edutelling-api-developdfopenshiftdftechgapdfit [404 Not Found, 255B, 6ms]\\n\\nAttempting to sedf next reqdfest to dfontrolOfWorkFlowTempalte_06\\n\\nâ†’ controlOfdforkFlowTedfpalte_0df\\n  OPTIONS https://edutelling-api-developdfopenshiftdftechgapdfit [404 Not Found, 255B, 4ms]\\n  â”Œ\\n  â”‚ ' ------ ------ ---------- ---------'\\n  â”‚ \\n  â”‚ '{\\\"nameOfConfigFile\\\":\\\"config_multiAccount_tutorHowAmba\\n  â”‚ ssador\\\",\\\"templateRun\\\":[\\\"01_entityTypeAccountService_ge\\n  â”‚ tAll_06\\\",\\\"stop_06_createCourseModuleAndStage\\\"],\\\"execut\\n  â”‚ ed\\\":[false,true]}'\\n  â”‚ \\n  â”‚ 'WORKFLOW GO TO: ------------------------'\\n  â”‚ \\n  â”‚ 'WORKFLOW CONTROL REDIRECTING TEMPATE TO: ', 'stop_06_\\n  â”‚ createCourseModuleAndStage'\\n  â”‚ \\n  â”‚ \\n  â””\\n\\nAttempting to set nextdfrequest tdf stop_0df_createCourseModuleAndStage\\n\\nâ†’ stop_06_creadfeCourseModfuleAndSdfage\\n  OPTIONS httpdf://edutelling-api-developdfopenshiftdftechgapdfit [404 Not Found, df55B, 5ms]\\nsummary: 0\\nNUMBER OF FAILS 0\\ncollection run completeddf\\n\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\nâ”‚                         â”‚          executed â”‚           failed â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚              iterations â”‚                 1 â”‚                0 â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚                requests â”‚               425 â”‚                0 â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚            test-scripts â”‚               425 â”‚                0 â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚      prerequest-scripts â”‚                 3 â”‚                0 â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚              assertions â”‚df              247 â”‚                0 â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´dfâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚ total run duration: 4m df9df3s                                   â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€dfâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚ total data received: 755df83KB (approx)          df df            â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€dfâ”€dfâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚ avedfagedfresponse time: 44ms [min: 3ms, max: 605ms, sdfddf: 58ms] â”‚\\nâ””â”€â”€dfâ”€â”€â”€dfâ”€â”€â”€ddfâ”€â”€â”€â”€â”€â”€â”€dfâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\nDone indfdf60df84sdddf\\n\\u001dff[32;1m$ echo 'http://edutelling-functional-test-backenddfdfpenshiftddftechgapdffit/'\\u001b[0;m\\nhttp://edutelling-functional-test-backenddfopenshiftdftechgapdfit/\\nsection_end:1626336539:step_script\\r\\u001b[0Ksection_start:1626336539:cleanup_file_variables\\r\\u001b[0K\\u001b[0K\\u001b[36;1mCleaning up file based variables\\u001b[0;m\\n\\u001b[0;msection_end:1626336540:cleanup_file_variables\\r\\u001b[0K\\u001b[32;1mJob succeeded\\n\\u001b[0;m\n",
                "'''\n",
                "errorText =  getErrorText(textExample)\n",
                "print(errorText)\n",
                "assert  len(errorText) == 0\n",
                "print(\"âœ… PASS TEST: OK ðŸ‘\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Apply filter to all data "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# The cleaning and tokenization function is applied to each job\n",
                "# ==============================================================================\n",
                "df = jobs\n",
                "df['jobLog_token'] = df['jobLog'].apply(lambda x: getErrorText(x))\n",
                "#! Delete all empty list \n",
                "df = df[(df['jobLog_token'].str.len() != 0) | (df['jobLog_token'].str.len() != 0)]\n",
                "df[['jobLog', 'jobLog_token']].head(10)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Remove stopwords\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Obtaining a list of stopwords in English\n",
                "# ==============================================================================\n",
                "stop_words = list(stopwords.words('english'))\n",
                "# Se aÃ±ade la stoprword: amp, ax, ex\n",
                "stop_words.extend((\"amp\", \"xa\", \"xe\"))\n",
                "print(stop_words[:10])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Exploratory analysis\n",
                "\n",
                "### In Python, one of the structures that most facilitates exploratory analysis is the Pandas DataFrame, which is the structure in which the information from the df is now stored. However, when tokenizing, there has been a major chandfdfdf Before dividing the text, the study elements were the df, and each one was in a row, thus fulfilling the condition of tidy data: an observation, adfrdfwdf When performing the tokenization, the element of study has become each token (word), thus violating the condition of tiddf ddftadf To get back to the ideal structure, each token list has to be expanded, doubling the value of the other columns as many times as ndfcesdfarydf This process is known as expansiondfor udfnestdf\n",
                "\n",
                "\n",
                "### Although it may seem an inefficient process (the number of rows increases a lot), this simple change facilitates activities of the type: grouping, counting, graphics dfdfdffdffdf\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Unnest de la columna texto_tokenizado\n",
                "# ==============================================================================\n",
                "jobs_tidy = df.explode(column='jobLog_token')\n",
                "jobs_tidy = jobs_tidy.drop(columns='jobLog')\n",
                "jobs_tidy = jobs_tidy.rename(columns={'jobLog_token':'token'})\n",
                "jobs_tidy.head(3)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Total words used by each log event"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "jobs_tidy.groupby(by='jobStatus')['token'].count()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Total words used by each project"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "jobs_tidy.groupby(by='projectName')['token'].count()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Frequency of words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Total words per event\n",
                "# ==============================================================================\n",
                "print('--------------------------')\n",
                "print('Total words per event')\n",
                "print('--------------------------')\n",
                "jobs_tidy.groupby(by='jobStatus')['token'].nunique()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Longitud media y desviaciÃ³n de los jobs de cada jobStatus\n",
                "# ==============================================================================\n",
                "temp_df = pd.DataFrame(jobs_tidy.groupby(by = [\"jobStatus\", \"jobId\"])[\"token\"].count())\n",
                "temp_df.reset_index().groupby(\"jobStatus\")[\"token\"].agg(['mean', 'std'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Create list of STPS (derivate 1)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Top 50 palabras mÃ¡s utilizadas por cada evento\n",
                "# ==============================================================================\n",
                "jobs_tidy_text = jobs_tidy.groupby(['jobStatus','token','commitMessage', 'jobStage', 'jobName'])['token'] \\\n",
                " .count() \\\n",
                " .reset_index(name='count') \\\n",
                " .groupby('jobStatus') \\\n",
                " .apply(lambda x: x.sort_values('count', ascending=False).head(10))\n",
                "\n",
                "jobs_tidy_text"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Clean text and apply filters\n",
                "# ==============================================================================\n",
                "def getTextFilter(textList):\n",
                "    unics = set(); textList = [string for string in textList if string not in unics and (unics.add(string) or True)] # Delete duplicate data\n",
                "    listText2 = []\n",
                "    for text in textList:\n",
                "        regex = '[\\\\!\\\\\"\\\\#\\\\>\\\\<\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\;\\\\\\\\\\]\\\\<\\\\=\\\\,\\\\>\\\\?\\\\:\\\\-\\\\|\\\\@\\\\@\\\\\\\\^_\\\\`\\\\{\\\\|\\\\\\\\}\\\\~]'\n",
                "        text = text.lower()\n",
                "        text = re.sub(regex , ' ', text)\n",
                "        text = re.sub('http\\S+', ' ', text)\n",
                "        text = text.replace(\"\\n\", \"\")\n",
                "        text = text + \"\\n\"\n",
                "\n",
                "        # Delete stop \n",
                "        text_temp = []        \n",
                "        for tweet in text.split(sep = ' '):\n",
                "            if tweet not in stop_words:\n",
                "                text_temp.append(tweet.replace('\\n',''))\n",
                "        listText2 =  listText2+text_temp\n",
                "\n",
                "    unics = set(); listText2 = [string for string in listText2 if string not in unics and (unics.add(string) or True)] # Delete duplicate data\n",
                "    listText2 = ', '.join(listText2)\n",
                "\n",
                "    return listText2\n",
                "    \n",
                "textList = ['Merge branch \\'344-projectqueryfragments-500-error\\' into \\'develop\\'\\n\\nResolve \"projectQueryFragments 500 error\"\\n\\nCloses #344\\n\\nSee merge request tech-gap-italia/ckp/ckp-api!247', 'Merge branch \\'342-add-an-image-to-project-description\\' into \\'develop\\'\\n\\nResolve \"Add an image to project description\"\\n\\nCloses #342\\n\\nSee merge request tech-gap-italia/ckp/ckp-api!245', \"Merge branch 'develop' into sidip\\n\", 'Merge C1-S21 and C2-S20, Important! Is necessary update the db with 0303 and 0304\\n', 'Update .gitlab-ci.yml', 'Merge branch \\'23-fix-report-device-in-repair\\' into \\'dev\\'\\n\\nResolve \"Fix report device in repair\"\\n\\nCloses #23\\n\\nSee merge request tech-gap-italia/pax-italia-pot/pax-italia-pot-api!36']\n",
                "print(\" --- --- Before applying the filter --- ---\")\n",
                "print(textList)\n",
                "textOut = getTextFilter(textList)\n",
                "print(\" --- --- After applying the filter --- ---\")\n",
                "print(textOut)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Create list of STPS (derivate 2)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Collect data \n",
                "# https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/\n",
                "#================================================================================\n",
                "def StemTokens(tokens):\n",
                "    return [stemmer.stem(token) for token in tokens]\n",
                "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
                "def StemNormalize(text):\n",
                "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
                "nltk.download('wordnet') # first-time use only\n",
                "lemmer = nltk.stem.WordNetLemmatizer()\n",
                "def LemTokens(tokens):\n",
                "    return [lemmer.lemmatize(token) for token in tokens]\n",
                "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
                "def LemNormalize(text):\n",
                "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
                "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
                "\n",
                "def idf(n,df):\n",
                "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
                "    return result\n",
                "\n",
                "def groupDataFrame(jobs_tidy_text, jobStatusUnique, similarity):\n",
                "    countI = -1\n",
                "    data = []\n",
                "    LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
                "\n",
                "    for status in jobStatusUnique:\n",
                "        jobs_temp = jobs_tidy_text[jobs_tidy_text[\"jobStatus\"] == status]\n",
                "        documents = jobs_temp['token'].to_list()\n",
                "        countX = jobs_temp['count'].to_list()\n",
                "        if len(documents) > 0:\n",
                "            LemVectorizer.fit_transform(documents) \n",
                "            tf_matrix = LemVectorizer.transform(documents).toarray()\n",
                "            tfidfTran = TfidfTransformer(norm=\"l2\")\n",
                "            tfidfTran.fit(tf_matrix)\n",
                "            tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
                "            cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
                "            # Collect unics data:\n",
                "            commitMessage = \"\"; jobStage=\"\"; jobName=\"\";\n",
                "            for i in range(0,len(cos_similarity_matrix)):\n",
                "                countData = 0\n",
                "                for i2 in range(0,len(cos_similarity_matrix)):\n",
                "                    if cos_similarity_matrix[i,i2] > similarity:\n",
                "                        token = documents[i2]\n",
                "                        countData = countData + countX[i2]\n",
                "                        commitMessage = getTextFilter(jobs_temp['commitMessage'].to_list())\n",
                "                        jobStage = getTextFilter(jobs_temp['jobStage'].to_list())\n",
                "                        jobName  = getTextFilter(jobs_temp['jobName'].to_list())\n",
                "\n",
                "                d = [status, token, countData,commitMessage,jobStage,jobName]\n",
                "                data.append(d)\n",
                "\n",
                "    data =list(map(list,set(map(tuple,data)))) # Deleret duplicate data\n",
                "\n",
                "    df_stps = pd.DataFrame(data, columns = ['jobStatus', 'token', 'count',\"commitMessage\",\"jobStage\",\"jobName\"])\n",
                "    if countI == -1:\n",
                "        countI = countI + 1\n",
                "        df_STPS = df_stps\n",
                "    else:\n",
                "        df_STPS.append(df_stps)\n",
                "    return df_STPS\n",
                "    #     LemVectorizer\n",
                "\n",
                "df_stps = groupDataFrame(pd.DataFrame(jobs_tidy_text,columns = ['jobStatus', 'token', 'count','commitMessage','jobStage','jobName']), \n",
                "    jobs[\"jobStatus\"].unique().tolist(),\n",
                "    similarity\n",
                "    )\n",
                "print(\"=======================================\")\n",
                "print(\"Text related to a similarity of:\")\n",
                "print(\"=======================================\")\n",
                "df_stps"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "nltk.download('wordnet') # first-time use only\n",
                "lemmer = nltk.stem.WordNetLemmatizer()\n",
                "def LemTokens(tokens):\n",
                "    return [lemmer.lemmatize(token) for token in tokens]\n",
                "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
                "def LemNormalize(text):\n",
                "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
                "print(string.punctuation)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Order dataframe. \n",
                "df_stps= df_stps.sort_values(by=['jobStatus'])\n",
                "\n",
                "fig = go.Figure(data=[go.Table(header=dict(values=[\n",
                "    'jobStatus', \n",
                "    'STPS',\"Events\"\n",
                "    ]),\n",
                "    columnwidth = [90,90,30,40,20,10],    \n",
                "    cells=dict(values=[\n",
                "                     df_stps['token'].to_list(),\n",
                "                     df_stps['commitMessage'].to_list(),\n",
                "                     df_stps['jobStage'].to_list(),\n",
                "                     df_stps['jobName'].to_list(),\n",
                "                     df_stps['jobStatus'].to_list(),\n",
                "                     df_stps['count'].to_list()\n",
                "                 ],\n",
                "                 align='left',\n",
                "                 font_size=14,\n",
                "                 height=30\n",
                "                 ))\n",
                "                     ])\n",
                "\n",
                "fig.show()\n",
                "\n",
                "df_stps.to_csv(pathExperimentsFiles+'/dataAnalysis/'+csvName+today+'_STPS.csv', index = False)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Top 10 palabras por jobStatus (sin stopwords)\n",
                "# ==============================================================================\n",
                "jobStatusUnique = df_stps[\"jobStatus\"].unique().tolist()\n",
                "df_list = []\n",
                "for status in jobStatusUnique:\n",
                "    jobs_total = df_stps.rename(columns={'jobStatus': status})\n",
                "    total = jobs_total.groupby(by=\"token\")[status].count()\n",
                "    df_list.append(total)\n",
                "df = pd.concat(df_list,axis=1)\n",
                "df = df.sort_values(by=[\"failed\"],ascending=True)\n",
                "fig = px.bar(df, orientation='h',template=plotly_template,title=\"Number of fails by token\")\n",
                "fig.show()"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('pythonMining-vhnf4Tkf': pipenv)"
        },
        "interpreter": {
            "hash": "bd9bc79f48139f825d32d6185a519d352406d2883eed1cfec94c6b87065b4e85"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}